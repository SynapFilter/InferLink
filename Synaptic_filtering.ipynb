{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Synaptic_filtering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd46b5e368f04205942bb047e5e64438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db2f33cbb7404c3c84ec32acb46dd10f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ab67519608349f08f55b7be51b67ccc",
              "IPY_MODEL_3c451856ee164fefae0c13df7f744c8d",
              "IPY_MODEL_118f33a2adda4eaeb9b7291dd0d92a8b"
            ]
          }
        },
        "db2f33cbb7404c3c84ec32acb46dd10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ab67519608349f08f55b7be51b67ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_638cfca7531a4d099de3eb26564c6652",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eded4dd7a094ba3a1decfb19c2954af"
          }
        },
        "3c451856ee164fefae0c13df7f744c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e165125e52f94f2d8cc4e4201bd4cfdc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9b9f02a18d44855a7524443cbdfe4bf"
          }
        },
        "118f33a2adda4eaeb9b7291dd0d92a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da959facb0554d50a07f9fa58564abf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [00:06&lt;00:00, 22.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc80dbf29bb04bb9b5ac0fd6b9585ef8"
          }
        },
        "638cfca7531a4d099de3eb26564c6652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eded4dd7a094ba3a1decfb19c2954af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e165125e52f94f2d8cc4e4201bd4cfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9b9f02a18d44855a7524443cbdfe4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da959facb0554d50a07f9fa58564abf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc80dbf29bb04bb9b5ac0fd6b9585ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing TPU Client for Pytorch**\n",
        "\n",
        "This notebook is used to train the Models and is optimised for using PyTorch on Clout TPUs. For more information on how to use TPU on google colab, please see https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb    "
      ],
      "metadata": {
        "id": "Ef47rYeCdHbS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDRfIid8Yu9F",
        "outputId": "84290494-0c77-4072-8894-8d69557a3b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 1.8.2+cpu\n",
            "Uninstalling torch-1.8.2+cpu:\n",
            "  Successfully uninstalled torch-1.8.2+cpu\n",
            "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
            "Collecting torch==1.8.2+cpu\n",
            "  Using cached https://download.pytorch.org/whl/lts/1.8/cpu/torch-1.8.2%2Bcpu-cp37-cp37m-linux_x86_64.whl (169.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cpu) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cpu) (3.10.0.2)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.2+cpu which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.2+cpu which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.2+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.2+cpu\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch\n",
        "\n",
        "!pip install torch==1.8.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "# !pip install torch==1.10.0 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "1RRxWoQxdKbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, AutoMinorLocator\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pll\n",
        "import torch_xla.utils.serialization as xser\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets\n",
        "\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = xm.xla_device()\n",
        "\n",
        "torch_xla.core.xla_model.set_rng_state(1, device=device)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxih3K7kY5ZT",
        "outputId": "1d91b9f0-f454-45ba-b93c-1d3b4b5eceac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9c570878f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making 'Robustness_analysis' directory the root directory**"
      ],
      "metadata": {
        "id": "gnvyiYJKdNrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Robustness_analysis/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUuk20dGZsd7",
        "outputId": "6844939b-9833-414b-94ad-61d1b987fdbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Robustness_analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For loading MNIST and CIFAR10 datsets**\n",
        "\n",
        "The loadDatset method takes the following input variables:\n",
        ">- dataset_type (str): the dataset you want to load, accepts a case-insensitive string 'mnist' for the MNIST dataset and 'cifar10' for the CIFAR10 dataset\n",
        ">- download_ (bool): option to download datatset from torchvision.datasets\n",
        ">- params (int, int): consists of two parameters in a tuple (batch size of dataset, number of workers) \n"
      ],
      "metadata": {
        "id": "QWqYtepsdehv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDataset(dataset_type, download_, params):\n",
        "  # Configures training (and evaluation) parameters\n",
        "  dataset_type = dataset_type.upper()\n",
        "  flags = {}\n",
        "  flags['batch_size'] = params[0]\n",
        "  flags['num_workers'] = params[1]\n",
        "\n",
        "  if dataset_type == 'CIFAR10':\n",
        "    # Transformations for CIFAR10\n",
        "    trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=r\"Dataset\", train=True,\n",
        "                                                transform=trans, download=download_)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"Dataset\", train=False,\n",
        "                                                transform=trans, download=download_)\n",
        "  elif dataset_type == 'MNIST':\n",
        "    # Transformations for MNIST\n",
        "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) \n",
        "    train_dataset = torchvision.datasets.MNIST(root=r\"Dataset\", train=True,\n",
        "                                                transform=trans, download=download_)\n",
        "    test_dataset = torchvision.datasets.MNIST(root=\"Dataset\", train=False,\n",
        "                                                transform=trans, download=download_)\n",
        "  else:\n",
        "    # Transformations for CIFAR10 (default)\n",
        "    trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=r\"Dataset\", train=True,\n",
        "                                                transform=trans, download=download_)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=\"Dataset\", train=False,\n",
        "                                                transform=trans, download=download_)\n",
        "    \n",
        "  # for train set\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  # for test set \n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "  \n",
        "  return train_loader, test_loader\n",
        "\n",
        "trainLoader, testLoader = loadDataset(dataset_type='mnist', download_=False, params=(100, 1))"
      ],
      "metadata": {
        "id": "LnVXmTnyaBI7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading ImageNet Tiny dataset**\n",
        "\n",
        "This dataset is constructed using the Tiny ImageNet dataset from https://www.kaggle.com/c/tiny-imagenet.\n",
        "\n",
        "The default dataset of 90,000 training examples and 10,000 test examples. To split the dataset to 80,000 training and 20,000 test examples, load the current dataset and select 50 examples of each class from the training set and to add to the test set.\n",
        "\n",
        "loadimageNet accepts two parameters:\n",
        ">- path (str): the path of the dataset folader 'tiny-imagenet-200/complete_dataset'\n",
        ">- params (int, int): consists of two parameters in a tuple (batch size of dataset, number of workers)\n"
      ],
      "metadata": {
        "id": "w2UztUqndjFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageNetTiny(torch.utils.data.Dataset):\n",
        "  # Creating ImageNet class to orangise dataset\n",
        "  def __init__(self, data_dict, transform=None, target_transform=None):\n",
        "    self.targets = data_dict['labels']\n",
        "    self.data = data_dict['images']\n",
        "    self.classes = data_dict['Classes_dict']\n",
        "\n",
        "    self.label_keys = self.classes.keys()\n",
        "    self.label_values = self.classes.values()\n",
        "    \n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = self.data[idx]\n",
        "    label = self.targets[idx]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "      label = self.target_transform(label)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def save_obj(obj, name):\n",
        "  with open(name + '.pkl', 'wb') as f:\n",
        "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_imagenet(dir_path):\n",
        "  train_file_name = \"train_set\"\n",
        "  test_file_name = \"test_set\"\n",
        "\n",
        "  with (open(dir_path+train_file_name+ '.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            train_dict = (pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "  with (open(dir_path+test_file_name+ '.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            test_dict = (pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "  \n",
        "  return train_dict, test_dict\n",
        "\n",
        "def loadImageNetTiny(path, params):\n",
        "  flags = {}\n",
        "  flags['batch_size'] = params[0]\n",
        "  flags['num_workers'] = params[1]\n",
        "\n",
        "  train_dict, test_dict = load_imagenet(path)\n",
        "\n",
        "  trainset = ImageNetTiny(train_dict)\n",
        "  testset = ImageNetTiny(test_dict)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    trainset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=flags['batch_size'],\n",
        "    sampler=train_sampler,\n",
        "    num_workers=flags['num_workers'],\n",
        "    drop_last=True)\n",
        "\n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    testset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      testset,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  return train_loader, test_loader\n",
        "\n",
        "trainLoader, testLoader = loadImageNetTiny(path=\"Dataset/tiny-imagenet-200/complete_dataset/\", params=(100, 1))"
      ],
      "metadata": {
        "id": "XONXyg69aHTU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Networks**\n",
        "\n",
        "This cell is for loading the model for training. To load the correct model with input and output shapes configured to suit the dataset, the 'Model' class accepts two parameters:\n",
        ">- mode_type (str): The type of model you want to load, options:\n",
        ">>* ResNet-18 (default)\n",
        ">>* ResNet-50\n",
        ">>* SqueezeNet-v1.1\n",
        ">>* ShuffleNet V2 x1.0 \n",
        "\n",
        ">- data_type (str): The dataset you are using, options:\n",
        ">>* MNIST\n",
        ">>* CIFAR10\n",
        ">>* ImageNet Tiny"
      ],
      "metadata": {
        "id": "WB5ASYQBdlMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getModel(model_type, dataset_type):\n",
        "  model_type = model_type.lower()\n",
        "  dataset_type = dataset_type.lower()\n",
        "\n",
        "  if model_type  == 'resnet50':\n",
        "    if dataset_type == 'mnist':\n",
        "      # for loading ResNet-50 model for MNIST dataset \n",
        "      temp_model = torchvision.models.resnet50(num_classes=10)\n",
        "      temp_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "    elif dataset_type == 'cifar10':\n",
        "      # for loading ResNet-50 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.resnet50(num_classes=10)\n",
        "    elif dataset_type == 'imagenet': \n",
        "      # for loading ResNet-50 model for TimageNet Tiny dataset\n",
        "      temp_model = torchvision.models.resnet50(num_classes=200)\n",
        "    else:\n",
        "      # (default) loading ResNet-50 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.resnet50(num_classes=10)\n",
        "\n",
        "  elif model_type == 'squeezenet':\n",
        "    if dataset_type == 'mnist':\n",
        "      # for loading SqueezeNet-v1.1 model for MNIST dataset \n",
        "      temp_model = torchvision.models.squeezenet1_1(num_classes=10)\n",
        "      temp_model.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
        "    elif dataset_type == 'cifar10':\n",
        "      # for loading SqueezeNet-v1.1 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.squeezenet1_1(num_classes=10)\n",
        "    elif dataset_type == 'imagenet': \n",
        "      # for loading SqueezeNet-v1.1 model for TimageNet Tiny dataset\n",
        "      temp_model = torchvision.models.squeezenet1_1(num_classes=200)\n",
        "    else:\n",
        "      # (default) loading SqueezeNet-v1.1 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.squeezenet1_1(num_classes=10)\n",
        "\n",
        "  elif model_type == 'shufflenet':\n",
        "    if dataset_type == 'mnist':\n",
        "      # for loading ShuffleNet V2 x1.0 model for MNIST dataset \n",
        "      temp_model = torchvision.models.shufflenet_v2_x0_5(num_classes=10)\n",
        "      temp_model.conv1[0] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "    elif dataset_type == 'cifar10':\n",
        "      # for loading ShuffleNet V2 x1.0 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.shufflenet_v2_x0_5(num_classes=10)\n",
        "    elif dataset_type == 'imagenet': \n",
        "      # for loading ShuffleNet V2 x1.0 model for TimageNet Tiny dataset\n",
        "      temp_model = torchvision.models.shufflenet_v2_x0_5(num_classes=200)\n",
        "    else:\n",
        "      # (default) loading ShuffleNet V2 x1.0 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.shufflenet_v2_x0_5(num_classes=10)\n",
        "\n",
        "  else:\n",
        "    # (Default) loading ResNet-18 models\n",
        "    if dataset_type == 'mnist':\n",
        "      # for loading ResNet-18 model for MNIST dataset \n",
        "      temp_model = torchvision.models.resnet18(num_classes=10)\n",
        "      temp_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "    elif dataset_type == 'cifar10':\n",
        "      # for loading ResNet-18 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.resnet18(num_classes=10)\n",
        "    elif dataset_type == 'imagenet': \n",
        "      # for loading ResNet-18 model for TimageNet Tiny dataset\n",
        "      temp_model = torchvision.models.resnet18(num_classes=200)\n",
        "    else:\n",
        "      # (default) loading ResNet-18 model for CIFAR10 dataset\n",
        "      temp_model = torchvision.models.resnet18(num_classes=10)\n",
        "\n",
        "  return temp_model\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, mod_type, data_type):\n",
        "    super().__init__()\n",
        "    self.model = getModel(mod_type, data_type)\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "##  @auto_move_data\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def training_step(self, batch, batch_no):\n",
        "    x, y = batch\n",
        "    logits = self(x)\n",
        "    loss = self.loss(logits, y)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(Model('resnet18', 'mnist'))\n",
        "model = WRAPPED_MODEL.to(device)\n"
      ],
      "metadata": {
        "id": "vxPzeFU4aLKq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading a pretrainned network**\n",
        "\n",
        "To load a pretrained network, change the varibles:\n",
        "\n",
        ">- model_name (str): name of the model you want to load (must be the save as the directory and file names of the saved model. Furthermore, the model must be the same as the one instantiated from the cell above.)\n",
        ">- dataset_name (str): the name of the dataset \n",
        ">- reload_init (str): The directory name of the initialisation you want to load ('Init_' followed by initialisation number)\n",
        ">- reload_epoch (int): The epoch of the pretrained model you want to load. "
      ],
      "metadata": {
        "id": "qi27ud9Xd6A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name, dataset_name = 'ResNet18', 'MNIST'\n",
        "reload_init, reload_epoch = 'Init_1', 15\n",
        "\n",
        "path = \"Saved_models/\"+model_name+\"/\"+dataset_name+\"/\" + reload_init + '/'+ model_name+\"_\"+dataset_name+\"_\" + str(reload_epoch) + \"epochs.ckpt\"\n",
        "# checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(Model('resnet18', 'mnist'))\n",
        "load_model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "load_model.load_state_dict(checkpoint)\n",
        "\n",
        "load_model = load_model.to(device)"
      ],
      "metadata": {
        "id": "YpP862aSaUz6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To run the FGSM attack on the model**"
      ],
      "metadata": {
        "id": "IpnyP4RJhNh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "  min_val, max_val = torch.min(image), torch.max(image)\n",
        "  # Collect the element-wise sign of the data gradient\n",
        "  sign_data_grad = data_grad.sign()\n",
        "  # Create the perturbed image by adjusting each pixel of the input image\n",
        "  perturbed_image = image + epsilon*sign_data_grad\n",
        "\n",
        "  # Adding clipping to maintain [0,1] range\n",
        "  # perturbed_image = torch.clamp(perturbed_image, min_val, max_val)\n",
        "  # Return the perturbed image\n",
        "  return perturbed_image\n",
        "\n",
        "dataset_name = 'MNIST'\n",
        "model_name= 'ResNet18'\n",
        "reload_init = 'Init_1'\n",
        "# filter_type = \"maxmin\"\n",
        "epoch_str = str(20)\n",
        "\n",
        "model_test = Model(model_name, dataset_name)\n",
        "path_dir = 'Saved_models/'+model_name+'/'+dataset_name+'/'\n",
        "path = path_dir+reload_init+'/'+model_name+'_'+dataset_name+'_'+epoch_str+'epochs.ckpt'\n",
        "\n",
        "checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "model_test.load_state_dict(checkpoint)\n",
        "model_test = model_test.to(device)\n",
        "# print(run_fgsm_attack(model_test, test_loader, 0.011))\n",
        "\n",
        "eps = 1.5\n",
        "\n",
        "adv_examples = []\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "correct_adv, correct_og, total, running_loss = 0, 0, 0, 0\n",
        "\n",
        "para_test_loader = pll.ParallelLoader(testLoader, [device]).per_device_loader(device)\n",
        "# Loop over all examples in test set\n",
        "for data, target in tqdm(para_test_loader):\n",
        "  # Send the data and label to the device\n",
        "  data, target = data.double().to(device), target.to(device)\n",
        "\n",
        "  # Set requires_grad attribute of tensor. Important for Attack\n",
        "  data.requires_grad = True\n",
        "\n",
        "  # Forward pass the data through the model\n",
        "  output = model_test(data)# .to(device)\n",
        "  _, init_pred = torch.max(output.data, 1)\n",
        "\n",
        "  # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "  # if init_pred.item() != target.item():\n",
        "  #   continue\n",
        "\n",
        "  # Calculate the loss\n",
        "  loss = criterion(output, target)\n",
        "  loss = loss.to(device)\n",
        "\n",
        "  # Zero all existing gradients\n",
        "  model_test.zero_grad()\n",
        "\n",
        "  # Calculate gradients of model in backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Collect datagrad\n",
        "  data_grad = data.grad.data\n",
        "\n",
        "  # Call FGSM Attack\n",
        "  perturbed_data = fgsm_attack(data, eps, data_grad)\n",
        "\n",
        "  # Re-classify the perturbed image\n",
        "  output = model_test(perturbed_data).to(device)\n",
        "  test_loss = criterion(output, target).to(device)\n",
        "\n",
        "  # Check for success\n",
        "  _, final_pred = torch.max(output.data, 1)\n",
        "\n",
        "  running_loss += test_loss.item() * data.size(0)\n",
        "  total += target.size(0)\n",
        "  correct_adv += (final_pred == target).sum().item()\n",
        "  correct_og += (init_pred == target).sum().item()\n",
        "\n",
        "# Calculate final accuracy for this epsilon\n",
        "final_acc_adv = correct_adv / total\n",
        "final_acc_og = correct_og / total\n",
        "epoch_loss = running_loss / total\n",
        "\n",
        "print(\"Clean network accuracy: \", final_acc_og)\n",
        "print(\"Adversarial network accuracy: \", final_acc_adv, \"for epsilon: \", eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "bd46b5e368f04205942bb047e5e64438",
            "db2f33cbb7404c3c84ec32acb46dd10f",
            "9ab67519608349f08f55b7be51b67ccc",
            "3c451856ee164fefae0c13df7f744c8d",
            "118f33a2adda4eaeb9b7291dd0d92a8b",
            "638cfca7531a4d099de3eb26564c6652",
            "8eded4dd7a094ba3a1decfb19c2954af",
            "e165125e52f94f2d8cc4e4201bd4cfdc",
            "c9b9f02a18d44855a7524443cbdfe4bf",
            "da959facb0554d50a07f9fa58564abf9",
            "bc80dbf29bb04bb9b5ac0fd6b9585ef8"
          ]
        },
        "id": "JvyEmWAZfpW-",
        "outputId": "afe303b2-7c2c-401b-d565-4e18d277c592"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd46b5e368f04205942bb047e5e64438",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean network accuracy:  0.9912\n",
            "Adversarial network accuracy:  0.5566 for epsilon:  1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Local analysis**"
      ],
      "metadata": {
        "id": "3IcF48OQ8wFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "  min_val, max_val = torch.min(image), torch.max(image)\n",
        "  # Collect the element-wise sign of the data gradient\n",
        "  sign_data_grad = data_grad.sign()\n",
        "  # Create the perturbed image by adjusting each pixel of the input image\n",
        "  perturbed_image = image + epsilon*sign_data_grad\n",
        "\n",
        "  # Adding clipping to maintain [0,1] range\n",
        "  # perturbed_image = torch.clamp(perturbed_image, min_val, max_val)\n",
        "  # Return the perturbed image\n",
        "  return perturbed_image\n",
        "\n",
        "\n",
        "def find_minmax(_obj, _names):\n",
        "  g_max = 0\n",
        "  g_min = 0\n",
        "\n",
        "  for name in _names:\n",
        "    temp_max = torch.max(_obj[name])\n",
        "    temp_min = torch.min(_obj[name])\n",
        "\n",
        "    if temp_max >= g_max:\n",
        "      g_max = temp_max\n",
        "\n",
        "    if temp_min <= g_min:\n",
        "      g_min = temp_min\n",
        "\n",
        "  return g_min, g_max\n",
        "\n",
        "\n",
        "def get_test_names(ex_model_og):\n",
        "  ex_model = copy.deepcopy(ex_model_og)\n",
        "  # params, names = count_parameters(ex_model.model)\n",
        "\n",
        "  model_obj = copy.deepcopy(ex_model.model.state_dict())\n",
        "  new_names = []\n",
        "\n",
        "  for p_name, module in list(ex_model.model.named_modules())[1:]:\n",
        "    nn_ = p_name + \".weight\"\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "      new_names.append(nn_)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "      new_names.append(nn_)\n",
        "\n",
        "  return new_names, model_obj\n",
        "\n",
        "\n",
        "def step_filter_func(input_mat, offset, fil_type):\n",
        "  y = torch.zeros_like(input_mat)\n",
        "  with torch.no_grad():\n",
        "    if fil_type == \"minmax\":\n",
        "      input_mat_fin = torch.where((input_mat <= offset), y, input_mat)\n",
        "      # input_mat[ind] *= 0\n",
        "    elif fil_type == \"maxmin\":\n",
        "      input_mat_fin = torch.where((input_mat >= offset), y, input_mat)\n",
        "    else:\n",
        "      input_mat_fin = torch.where((input_mat <= offset), y, input_mat)\n",
        "      \n",
        "  return input_mat_fin\n",
        "\n",
        "def count_zero(x):\n",
        "  if torch.is_tensor(x):\n",
        "    return torch.count_nonzero(x==0)\n",
        "  else:\n",
        "    return np.count_nonzero(x==0)\n",
        "\n",
        "def save_obj(obj, name):\n",
        "  with open(name + '.pkl', 'wb') as f:\n",
        "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "epochs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "epsilon = 0.03\n",
        "samples_no = 25 \n",
        "dataset_name = 'MNIST'\n",
        "model_name= 'ResNet18'\n",
        "filter_type = \"minmax\"\n",
        "rep = [\"advog\", \"advog2\", \"advog3\"]\n",
        "glob_min, glob_max = -1, 1\n",
        "\n",
        "final_dict = {}\n",
        "\n",
        "for r, rep_count in enumerate(rep):\n",
        "  r += 1\n",
        "  for e in tqdm(epochs):\n",
        "    epoch_str = str(e)\n",
        "    WRAPPED_MODEL = xmp.MpModelWrapper(Model(model_name, dataset_name))\n",
        "    model_test = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    path_dir = 'Saved_models/'+model_name+'/'+dataset_name+'/'\n",
        "    path = path_dir+'Init_'+str(r)+'/'+model_name+'_'+dataset_name+'_'+epoch_str+'epochs.ckpt'\n",
        "\n",
        "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "    model_test.load_state_dict(checkpoint)\n",
        "    model_test = model_test.to(device)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    acc_og_list, acc_adv_list, lss_list, zero_count = [], [], [], []\n",
        "    results_dict = {}\n",
        "\n",
        "    names_, obj_ = get_test_names(model_test)\n",
        "    # g_min, g_max = find_minmax(obj_, names_)\n",
        "    \n",
        "    if filter_type == \"minmax\":\n",
        "      g_min, g_max = glob_min, glob_max\n",
        "    elif filter_type == \"maxmin\":\n",
        "      g_min, g_max = glob_max, glob_min\n",
        "    elif filter_type == 'pulseminmax':\n",
        "      g_min, g_max = glob_min, glob_max\n",
        "    else:\n",
        "      g_min, g_max = glob_min, glob_max\n",
        "\n",
        "    alpha_vals = torch.linspace(g_min, g_max, samples_no, device=device)\n",
        "    steps = float((g_max - g_min) / samples_no)\n",
        "\n",
        "    for a_count, alpha in enumerate(alpha_vals):\n",
        "      temp_obj = copy.deepcopy(obj_)\n",
        "      temp_model = copy.deepcopy(model_test)\n",
        "      z_count = 0\n",
        "\n",
        "      prev = alpha + steps\n",
        "      next = alpha - steps\n",
        "      prev, next = prev.to(device), next.to(device)\n",
        "\n",
        "      for name in names_:\n",
        "        mat = copy.deepcopy(temp_obj[name])\n",
        "        mat = mat.to(device)\n",
        "        filtered_weight = torch.zeros_like(mat)\n",
        "\n",
        "        if filter_type == \"pulseminmax\":\n",
        "          maska = mat >= prev\n",
        "          maskb = mat < next\n",
        "\n",
        "          filtered_weight[~maska] = mat[~maska]\n",
        "          filtered_weight[~maskb] = mat[~maskb]\n",
        "\n",
        "          z_count += count_zero(filtered_weight)\n",
        "\n",
        "        else:\n",
        "          filtered_weight = step_filter_func(mat, alpha, filter_type)\n",
        "          z_count += count_zero(filtered_weight)\n",
        "\n",
        "      temp_obj[name] = filtered_weight\n",
        "      zero_count.append(int(z_count))\n",
        "      temp_model.model.load_state_dict(temp_obj)\n",
        "\n",
        "      # file_path_final = path_dir+\"temp_model.ckpt\"\n",
        "      # # torch.save(model.state_dict(), file_path_final)\n",
        "      # xm.save(temp_model.state_dict(), file_path_final)\n",
        "\n",
        "      # checkpoint = torch.load(path_dir+\"temp_model.ckpt\", map_location=torch.device('cpu'))\n",
        "\n",
        "      adv_examples = []\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      criterion = criterion.to(device)\n",
        "\n",
        "      correct_adv, correct_og, total, running_loss = 0, 0, 0, 0\n",
        "      para_test_loader = pll.ParallelLoader(testLoader, [device]).per_device_loader(device)\n",
        "\n",
        "      for data, target in para_test_loader:\n",
        "        data, target = data.double().to(device), target.to(device)\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = temp_model(data)\n",
        "        _, init_pred = torch.max(output.data, 1)\n",
        "        init_pred.to(device)\n",
        "\n",
        "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "        # if init_pred.item() != target.item():\n",
        "        #   continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        temp_model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = temp_model(perturbed_data).to(device)\n",
        "        test_loss = criterion(output, target)\n",
        "\n",
        "        # Check for success\n",
        "        _, final_pred = torch.max(output.data, 1)\n",
        "        final_pred.to(device)\n",
        "\n",
        "        running_loss += test_loss.item() * data.size(0)\n",
        "        total += target.size(0)\n",
        "        correct_adv += (final_pred == target).sum().item()\n",
        "        correct_og += (init_pred == target).sum().item()\n",
        "      \n",
        "      acc_og = correct_og/total\n",
        "      acc_adv = correct_adv/total\n",
        "      lss_ = running_loss/total\n",
        "      \n",
        "      acc_og_list.append(acc_og)\n",
        "      acc_adv_list.append(acc_adv)\n",
        "      lss_list.append(lss_)\n",
        "\n",
        "      alpha = float(alpha)\n",
        "      # results_dict[alpha] = (acc_og, acc_adv, lss_, zero_count)\n",
        "      del temp_model\n",
        "\n",
        "    final_dict[e] = [acc_og_list, acc_adv_list, lss_list, zero_count]\n",
        "\n",
        "  print(final_dict.keys())\n",
        "  dict_save_name = path_dir+\"/global_data/\"+model_name+\"_\"+rep_count+\"_global_\"+filter_type+\"_\"+dataset_name+\"_\"+\"allepochs\"\n",
        "  save_obj(final_dict, dict_save_name)\n"
      ],
      "metadata": {
        "id": "VJmUwDKYbqiR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GLobal analysis**"
      ],
      "metadata": {
        "id": "wO5wdZn580RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "  min_val, max_val = torch.min(image), torch.max(image)\n",
        "  # Collect the element-wise sign of the data gradient\n",
        "  sign_data_grad = data_grad.sign()\n",
        "  # Create the perturbed image by adjusting each pixel of the input image\n",
        "  perturbed_image = image + epsilon*sign_data_grad\n",
        "\n",
        "  # Adding clipping to maintain [0,1] range\n",
        "  # perturbed_image = torch.clamp(perturbed_image, min_val, max_val)\n",
        "  # Return the perturbed image\n",
        "  return perturbed_image\n",
        "\n",
        "\n",
        "def get_test_names(ex_model_og):\n",
        "  ex_model = copy.deepcopy(ex_model_og)\n",
        "  # params, names = count_parameters(ex_model.model)\n",
        "\n",
        "  model_obj = copy.deepcopy(ex_model.model.state_dict())\n",
        "  new_names = []\n",
        "\n",
        "  for p_name, module in list(ex_model.model.named_modules())[1:]:\n",
        "    nn_ = p_name + \".weight\"\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "      new_names.append(nn_)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "      new_names.append(nn_)\n",
        "\n",
        "  return new_names, model_obj\n",
        "\n",
        "\n",
        "def step_filter_func(input_mat, offset, fil_type):\n",
        "  y = torch.zeros_like(input_mat)\n",
        "  with torch.no_grad():\n",
        "    if fil_type == \"minmax\":\n",
        "      input_mat_fin = torch.where((input_mat <= offset), y, input_mat)\n",
        "      # input_mat[ind] *= 0\n",
        "    elif fil_type == \"maxmin\":\n",
        "      input_mat_fin = torch.where((input_mat >= offset), y, input_mat)\n",
        "    else:\n",
        "      input_mat_fin = torch.where((input_mat <= offset), y, input_mat)\n",
        "      \n",
        "  return input_mat_fin\n",
        "\n",
        "\n",
        "def save_obj(obj, name):\n",
        "  with open(name + '.pkl', 'wb') as f:\n",
        "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def count_zero(x):\n",
        "  if torch.is_tensor(x):\n",
        "    return torch.count_nonzero(x==0)\n",
        "  else:\n",
        "    return np.count_nonzero(x==0)\n",
        "\n",
        "\n",
        "device = xm.xla_device()\n",
        "\n",
        "epochs = [60, 70, 80, 90, 100]\n",
        "dataset_name = \"ImageNet\"\n",
        "# epsilon = 0.77 # (0.38) MNIST ResNet18, (0.75) MNIST ResNet50 \n",
        "epsilon = 0.5 # 0.035 # CIFAR10 ResNet18 (0.015), ResNet50 (0.02), MNIST_SqueezeNet (0.8)\n",
        "filter_type = \"minmax\"\n",
        "filter_folder = 'Step_'+filter_type+'/'\n",
        "model_name = \"ShuffleNet\"\n",
        "\n",
        "for e_no in epochs:\n",
        "  print(\"\\n\")\n",
        "  print(\"running experiment for epoch: \", e_no)\n",
        "  path = 'Saved_models/'+model_name+'/' + dataset_name + '/'\n",
        "  file_name = 'Init_3/'+model_name+'_' + dataset_name + \"_\" + str(e_no) + 'epochs.ckpt'\n",
        "\n",
        "  checkpoint = torch.load(path+file_name, map_location=torch.device(\"cpu\"))\n",
        "  # checkpoint = xser.load(path+file_name)\n",
        "  model_ = Model()\n",
        "  model_.load_state_dict(checkpoint)\n",
        "  model_.to(device)\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  mod_names, mod_obj = get_test_names(model_)\n",
        "  samples_no = 25\n",
        "  results_dict = {}\n",
        "\n",
        "  for name in tqdm(mod_names):\n",
        "    weight = copy.deepcopy(mod_obj[name])\n",
        "    steps = ((torch.min(weight) - torch.max(weight)) / samples_no) / 2\n",
        "\n",
        "    if filter_type == \"minmax\":\n",
        "      alpha_vals = torch.linspace(torch.min(weight), torch.max(weight), samples_no, device=device)\n",
        "    elif filter_type == \"maxmin\":\n",
        "      alpha_vals = torch.linspace(torch.max(weight), torch.min(weight), samples_no, device=device)\n",
        "    else:\n",
        "      alpha_vals = torch.linspace(torch.min(weight), torch.max(weight), samples_no, device=device)\n",
        "\n",
        "    acc_og_list, acc_adv_list, lss_list, zero_count = [], [], [], []\n",
        "\n",
        "    # prev = torch.min(alpha_vals) - steps\n",
        "    # next = prev\n",
        "\n",
        "    for alpha in alpha_vals:\n",
        "      model_clone = copy.deepcopy(model_)\n",
        "      mat = copy.deepcopy(weight)\n",
        "      mat = mat.to(device)\n",
        "      filtered_weight = torch.zeros_like(mat)\n",
        "      new_obj = copy.deepcopy(mod_obj)\n",
        "\n",
        "      prev = alpha + steps\n",
        "      next = alpha - steps\n",
        "      prev, next = prev.to(device), next.to(device)\n",
        "\n",
        "      if filter_type == \"pulseminmax\":\n",
        "        maska = mat >= prev\n",
        "        maskb = mat < next\n",
        "\n",
        "        filtered_weight[~maska] = mat[~maska]\n",
        "        filtered_weight[~maskb] = mat[~maskb]\n",
        "\n",
        "        # new_obj[name] = filtered_weight\n",
        "        # z_count += count_zero(filtered_weight)\n",
        "\n",
        "      else:\n",
        "        filtered_weight = step_filter_func(mat, alpha, filter_type)\n",
        "\n",
        "      zero_count.append(count_zero(filtered_weight))\n",
        "      # change this back to new_obj[name] (and declare new_obj at the beginning of the loop) do this for pulse analysis\n",
        "      new_obj[name] = filtered_weight\n",
        "      model_clone.model.load_state_dict(new_obj)\n",
        "      model_clone = model_clone.to(device)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      criterion = criterion.to(device)\n",
        "\n",
        "      correct_adv, correct_og, total, running_loss = 0, 0, 0, 0\n",
        "      para_test_loader = pll.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "\n",
        "      for data, target in para_test_loader:\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.double().to(device), target.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model_clone(data)# .to(device)\n",
        "        _, init_pred = torch.max(output.data, 1)\n",
        "\n",
        "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
        "        # if init_pred.item() != target.item():\n",
        "        #   continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        loss = loss.to(device)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model_.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect datagrad\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output_adv = model_clone(perturbed_data)# .to(device)\n",
        "        test_loss = criterion(output, target)# .to(device)\n",
        "\n",
        "        # Check for success\n",
        "        _, final_pred = torch.max(output_adv.data, 1)\n",
        "\n",
        "        running_loss += test_loss.item() * data.size(0)\n",
        "        total += target.size(0)\n",
        "        correct_adv += (final_pred == target).sum().item()\n",
        "        correct_og += (init_pred == target).sum().item()\n",
        "\n",
        "        del test_loss, target, data\n",
        "\n",
        "      acc_og = correct_og/total\n",
        "      acc_adv = correct_adv/total\n",
        "      lss_ = running_loss/total\n",
        "\n",
        "      acc_og_list.append(acc_og)\n",
        "      acc_adv_list.append(acc_adv)\n",
        "      lss_list.append(lss_)\n",
        "\n",
        "      del model_clone, filtered_weight, new_obj, para_test_loader\n",
        "\n",
        "    new_name = name.replace(\".weight\", \"\")\n",
        "    results_dict[new_name] = (acc_og_list, acc_adv_list, zero_count, alpha_vals)\n",
        "\n",
        "  dict_save_name = path+filter_folder+model_name+\"_step_\"+filter_type+\"_advog3_\" + dataset_name + \"_\" + str(e_no) + \"epochs\"\n",
        "  save_obj(results_dict, dict_save_name)\n"
      ],
      "metadata": {
        "id": "EyMmHFQVjJ9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}